{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT1mFC4NLgnk"
   },
   "source": [
    "### Наивный байесовский классфикатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoiNld1yLgnl"
   },
   "source": [
    "Многие из вас могли слышать про [теорему Байеса](http://mathprofi.ru/formula_polnoj_verojatnosti_formuly_bajesa.html) из курса матстата и теории вероятностей:\n",
    "\n",
    "$\\large P(A_i|B) = \\frac{P(A_iB)}{P(B)} =  \\frac{P(B \\mid A_i)\\, P(A_i)}{P(B)}$\n",
    "\n",
    "Кратко опишем смысл формулы:  \n",
    "\n",
    "_Пусть некое событие $B$ может наступить в результате осуществеления одной из гипотез $A_1$, $A_2$, $A_3$ и тд.  \n",
    "Зная вероятности гипотез $A$ до наступления события, можно, уже после свершения события $B$, вычислить, какая из гипотез привела к свершению этого события с наибольшей вероятностью._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HFKle3KLgnn"
   },
   "source": [
    "В примере выше у нас был только один признак (курение), но что делать, если признаков больше одного?  \n",
    "\n",
    "Формула изменится таким образом:\n",
    "\n",
    "$ P(A_i|B_1, B_2, ... , B_n) = \\frac{P(B_1, B_2, ... , B_n | A_i)*P(A_i)}{P(B)}$\n",
    "\n",
    "Условная вероятность в числителе расписывается на произведение вероятностей. Тут есть важный нюанс - мы наивно предполагаем, что признаки $B_1$, $B_2$, ... , $B_n$ независимы друг от друга, то есть они никак не коррелируют друг с другом:\n",
    "\n",
    "$ P(B_1, B_2, ... , B_n | A_i) = P(B_1 | A_i) * P(B_2 | A_i) *\\ ...\\ * P(B_n | A_i) $\n",
    "\n",
    "Значит, перед использованием классификатора в идеале необходимо проверить все признаки на взаимную корреляцию, и удалить сильно коррелирующие. Для поиска можно использовать [критерий Пирсона](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb-fUZMSLgnm"
   },
   "source": [
    "Априорные и апостериорные вероятности и так далее - это конечно интересно, но где тут связь с классификацией?! Вспомним типичную задачку на теорему байеса:  \n",
    "\n",
    "**_В выборке 40% мужчин и 60% женщин. Известно, что среди них курит 10% женщин и 70% мужчин. Про некоего человека N известно, что он курит. Мужчина он, или женщина?_**\n",
    "\n",
    "Данная задачка по сути есть классификация - у нас есть значение нецелевого признака (курение), и на основе этого нам надо предсказать пол. Решение задачки очень простое, так как все нужные нам числа прописаны сразу в условии:\n",
    "\n",
    "$P(M) = 0.4$  \n",
    "$P(Ж) = 0.6$  \n",
    "$P(K|M) = 0.7$ - вероятность того, что человек курит (К) если он мужчина.  \n",
    "$P(К|Ж) = 0.1$ - вероятность того, что человек курит, если он женщина.  \n",
    "\n",
    "Понятно, что является человек мужчиной или женщиной - это гипотезы, а вероятность того, что курит или не курит - событие, которое может произойти в результате наступления этих гипотез.  \n",
    "Рассчитаем полную вероятность того, что человек курит:\n",
    "$P(К) = P(M)*P(K|M) + P(Ж)*P(К|Ж) = 0.34$.\n",
    "\n",
    "Теперь мы сможем наконец предсказать пол человека зная, курит он (она?) или нет:\n",
    "\n",
    "$P(M|К) = \\frac{P(К|М)*P(M)}{P(К)} = 28/34$\n",
    "\n",
    "$P(Ж|К) = \\frac{P(К|Ж)*P(Ж)}{P(К)} = 6/34$\n",
    "\n",
    "Видно, что тут у нас есть вероятности всех классов. Выбираем просто класс с наибольшей вероятностью (argmax) - и это мужчина. Очевидный плюс такого классифкатора - не требуется большой объем данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PGX5TFJLgno"
   },
   "source": [
    "Любопытно, что если реализовывать классификатор \"в лоб\", по определению теоремы байеса, то точность его будет не высока, так как не учитывается то, каким образом распределены сами данные. Подробно про разные варианты байесовского классификатора можно почитать [тут](https://scikit-learn.org/stable/modules/naive_bayes.html). В данной работе мы будем реализовывать [несколько упрощенный мультиномиальный классификатор](http://bazhenov.me/blog/2012/06/11/naive-bayes.html). Настоятельно рекомендую **прочесть пример** по ссылке и разобраться, каким образом высчитываются все параметры. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51JEID56Lgno"
   },
   "source": [
    "### Пред-подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-CH-x9yLgnp"
   },
   "source": [
    "Для начала мы разберемся с данными, а затем реализуем классификатор.\n",
    "\n",
    "Говоря о самих данных, то мы попробуем решить задачу классификации [новостных текстов](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) - байесовские классификаторы часто используются в этой прикладной области машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANyLqj0XLgnp",
    "outputId": "d6d28c1d-5e8e-4b87-ef5f-f72d695f8eb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на данные - в этот раз датасет доступен напрямую из sklearn.\n",
    "# выведем все доступные категории.\n",
    "# параметр subset отвечает за разделенение данных (train - тренировочная выборка, test - тестовая).\n",
    "\n",
    "# параметр remove говорит о том, какие части данных нужно удалить, чтобы не допустить переобучения.\n",
    "# headers - заголовки новостных групп\n",
    "# quotes - удаление строк, похожих на цитаты из других источников\n",
    "# footers - удаление блоков из конца текста, похожих на подписи\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZengtNm_Lgns"
   },
   "source": [
    "Мы будем использовать только 4 класса текстов: `alt.atheism`, `sci.space`, `talk.religion.misc`, `comp.graphics`.  Используя параметр `categories` в функции `fetch_20newsgroups`, задайте список нужных нам категорий и разбейте данные на тренировочную и тестовые части (параметр `subset`). \n",
    "\n",
    "Учтите, что сами данные (целевые и нецелевые признаки) лежат в атрибутах `data` и `target`:\n",
    "```python\n",
    "subset = fetch_20newsgroups( ... )\n",
    "X = subset.data\n",
    "y = subset.target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "CEN5cAq3Lgns"
   },
   "outputs": [],
   "source": [
    "categories = ('alt.atheism', 'sci.space', 'talk.religion.misc', 'comp.graphics')\n",
    "\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    categories = categories)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    categories = categories)\n",
    "\n",
    "x_train = newsgroups_train.data\n",
    "x_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_GZP6zJLgnt"
   },
   "source": [
    "Посмотрим на типы данных: видно, что X - это список со строками, а y - просто массив с метками класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CGx11VSLgnt",
    "outputId": "1fbefee1-5413-4b8e-810d-3006b18911b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_train[0]))\n",
    "print(type(y_train))\n",
    "print(type(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3mGN3_ALgnu"
   },
   "source": [
    "Выведите на экран по 1 тексту из каждой категории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZTBntG4Lgnu",
    "outputId": "2e7902f9-a493-4dec-d63d-86c5f466a151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "sci.space: Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "talk.religion.misc: \n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "comp.graphics: \n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(len(categories)):print(f\"{categories[i]}: {x_train[np.where(y_train == i)[0][0]]}\\n{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q107gR1lLgnv"
   },
   "source": [
    "проведем небольшой эксперимент по отбору признаков: датасет изкоробочный, специально для обучения машинному обучению (sic), НО...  \n",
    "... проверим данные на наличие пробелов и пустых строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jk59sXkXLgnv",
    "outputId": "0fc1386b-2a9d-43c9-d2a4-55b53a4a1597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(x_train.count(''))\n",
    "print(x_train.count(' '))\n",
    "print(x_train.count('  '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8CxsEhrLgnw"
   },
   "source": [
    "Как мы видим, среди признаков внезапно оказались пустые строки, и этим пустым строкам присвоен класс! Очевидно, что такое недопустимо, поэтому датасет необходимо немного вычистить.  \n",
    "\n",
    "Заметьте, что мы делаем проверку на строку с 1м, 2мя, 3мя пробелами, но не с 5 пробелами и больше. Чтобы эффективно найти строки с некоторым неизвестным числом пробелов, чтоит воспользоваться регулярным выражением `^\\\\s*$` - данная регулярка срабатывает на строках, состоящих целиком из пробелов (`\\s` - это пробельный символ, квантификатор `*` указывает, что число повторений такого символа больше одного, символ `^` указывает на начало строки, а знак доллара- на конец строки).\n",
    "\n",
    "Для нахождения возьмем функцию `match` из библиотеки `re` регулярных выражений в питоне.  \n",
    "Доки по функции [match](https://docs.python.org/3/library/re.html#re.match). Обратите внимание, что она возвратит `None`, если паттерн не совпал с заданной строкой, или возвратит некий `match object`, если будет совпадение.  \n",
    "\n",
    "**Задача:** в тестовой и тренировочной выборках найти индексы пробельных строк. Зная индексы (это должен быть массив индексов), можно удалить такие элементы из тренировочной и тестовой выборок. Для удаления можете использовать логические маски, или [np.delete](https://numpy.org/doc/stable/reference/generated/numpy.delete.html)\n",
    "\n",
    "**Hints**\n",
    "- np.delete\n",
    "- re.match\n",
    "- '''^\\\\s*$'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly0SwGwkLgnw"
   },
   "source": [
    "Напечатаем число элементов до очистки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Im9ZV2OZLgnw",
    "outputId": "6e581bcb-6e71-41d1-90ec-33c2e75851b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034\n",
      "1353\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "VsPBJdA3Lgnx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "\n",
    "pattern = re.compile('^\\s*$')\n",
    "\n",
    "data = []\n",
    "for i in range(len(x_train)): \n",
    "    if(re.match(pattern,x_train[i])!=None): data.append(i)\n",
    "\n",
    "x_train = np.delete(x_train,data)\n",
    "y_train = np.delete(y_train,data)\n",
    "\n",
    "data = []\n",
    "for i in range(len(x_test)): \n",
    "    if(re.match(pattern,x_test[i])!=None): data.append(i)\n",
    "\n",
    "x_test = np.delete(x_test,data)\n",
    "y_test = np.delete(y_test,data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFfVJ7G9Lgny"
   },
   "source": [
    "Выведем число элементов после очистки, должно получиться 1977 и 1318 элементов в тренировочной и тестовой выборках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Fagvq0cDLgny"
   },
   "outputs": [],
   "source": [
    "assert len(y_train) == 1977\n",
    "assert len(x_train) == 1977\n",
    "assert len(y_test) == 1318\n",
    "assert len(x_test) == 1318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7Ce0CS80QNO"
   },
   "source": [
    "Посмотрим на то, что из себя представляют целевые признаки. Это целое число, обозначающее индекс категории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGxd_WiU0NkG",
    "outputId": "5f9184a3-d944-4fdb-cdae-8ac583e7f186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbacr8Dh0olZ"
   },
   "source": [
    "Преобразуем этот индекс в имя категории. Для этого воспользуемся генератором списков и методом `target_names`, который по индексу вернет нам название категории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "3sUndLwf0oDE"
   },
   "outputs": [],
   "source": [
    "y_test = [newsgroups_test.target_names[idx] for idx in y_test]\n",
    "y_train = [newsgroups_train.target_names[idx] for idx in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vaYpBsX2ADT",
    "outputId": "dd859327-6aca-426f-df9d-e0d1a9fda05f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2qqYzkLLgnz"
   },
   "source": [
    "### Как извлечь информацию из текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWsJ4rDvLgnz"
   },
   "source": [
    "В предыдущей части вы подготовили сам датасет, состоящий из строк с символами. Но каким образом конвертировать символы в числа, чтобы предложение образовало точку в некотором многомерном пространстве?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjV9C_21Lgnz"
   },
   "source": [
    "Для этого есть несколько способов:\n",
    "- Счетчики\n",
    "- Распределенная семантика\n",
    "\n",
    "**Счетчики** работают довольно просто - из слов всех предложений выборки вы формируете один большой словарь, и, получая новое предложение, просто считаете, сколько раз каждое слово из словаря было представлено в этом предложении. Таким образом, ваше предложение закодировано вектором, длина которого равна длине словаря. Недостатков у этого метода только два: получающийся вектор имеет очень большую длину, и к тому же он сильно разряжен (в нем много нулей, так как одно предложение априори не может содержать всех слов), из-за чего с ним трудно работать. Второй недостаток - мы просто смотрим на сам _факт наличия_ слова в предложении, но не на _контекст_ этого слова.\n",
    "\n",
    "Оба этих недостатка успешно забарывают методы, основанные на **распределенной семантике**, такие как легендарный Word2Vec и известный GloVe. Такие методы основаны на недо-нейронных сетях, поволяют учитывать контекст слова, и позволяют создавать вектора заданной пользователем длины. \n",
    "\n",
    "Проблема их в том, что значения таких векторов - это и отрицательные числа, а байесовская модель работает только с натуральными. К тому же, такие алгоритмы работают на уровне слова, а не предложения - у вас просто будет набор векторов, представляющих каждое слово из предложения, а каким образом представить вектор самого предложения с наименьшими потерями информации - вопрос.  Поэтому далее мы будем использовать именно счетчики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7ifetN0Lgn0"
   },
   "source": [
    "Для работы со счетчиками мы возьмем реализацию из библиотеки `sklearn`.  \n",
    "\n",
    "по ссылке [тут](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) можно почитать про сами счетчики (мешок слов и TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KvV7CvyLgn1"
   },
   "source": [
    "#### **Мешок слов**\n",
    "\n",
    "[Документация](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "\n",
    "Можно начать с очень простой идеи. Давайте разобъем все предложения на слова. Составим словарь всех слов, которые будут встречаться во всех  наших текстах. И отметим, встречается ли это слово в нашем конкретном примере. Другими словами, пусть в таблице в строках будут предложения, в столбцах - слова, а в ячейках число, которое показывает сколько раз это слово встречалось в этом предложении. Получается, что каждому объекту выборки будет сопоставлен вектор.\n",
    "\n",
    "Векторизацию мы делаем сразу методом `fit_transform` - он эквивалентен последовательному вызову \n",
    "```python\n",
    "bow = count_vectorizer.fit(data).transform(data)\n",
    "```\n",
    "\n",
    "Очевидно, что метод `fit` составляет словарь, а `transform` делает вектор из предложения, согласно имеющемуся словарю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVvPgtemLgn1",
    "outputId": "d1f8b38c-bea1-4c38-a694-7054e4478984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape= (3, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "texts = [\n",
    "    \"I've been searching for the right words to thank you for this breather.\",\n",
    "    \"You have been wonderful and a blessing at all times\",\n",
    "    \"I promise i wont take your help for granted and will fulfil my promise.\"\n",
    "]\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "print(\"Shape=\", bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFaq7qzZLgn2",
    "outputId": "c3fa5085-93b2-4ade-e637-2ece74f3bf3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 0,\n",
       " 'and': 1,\n",
       " 'at': 2,\n",
       " 'been': 3,\n",
       " 'blessing': 4,\n",
       " 'breather': 5,\n",
       " 'for': 6,\n",
       " 'fulfil': 7,\n",
       " 'granted': 8,\n",
       " 'have': 9,\n",
       " 'help': 10,\n",
       " 'my': 11,\n",
       " 'promise': 12,\n",
       " 'right': 13,\n",
       " 'searching': 14,\n",
       " 'take': 15,\n",
       " 'thank': 16,\n",
       " 'the': 17,\n",
       " 'this': 18,\n",
       " 'times': 19,\n",
       " 'to': 20,\n",
       " 've': 21,\n",
       " 'will': 22,\n",
       " 'wonderful': 23,\n",
       " 'wont': 24,\n",
       " 'words': 25,\n",
       " 'you': 26,\n",
       " 'your': 27}"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на словарь всех слов (метод vocabulary_)\n",
    "# число - это индекс слова в строке матрицы\n",
    "\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPPj1XurLgn3"
   },
   "source": [
    "Теперь составим ту самую матрицу, где в столбцах слова, а в строках тексты.\n",
    "\n",
    "Как мы видим, в первом и втором предложениях есть слово \"been\", а в третьем его нет (так как у \"been\" индекс равен 3).  \n",
    "Так как векторайзер возвращает разряженную матрицу, то воспользуемся методом `.toarray()`, чтобы превратить ее в numpy-массив."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03EpcKYYLgn4",
    "outputId": "3a8382b9-ae82-4cf6-f65a-7ee18e8e76ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ULPT499Lgn4"
   },
   "source": [
    "При векторизации можно удалить \"стоп-слова\" - они не несут какого-то смысла, но нужны для грамматики (параметр `stop_words`). Как мы видим, словарь стал заметно меньше, соответсвенно и вектор тоже стал короче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZWkXFwYLgn4",
    "outputId": "ced69a7f-8f95-43a6-a12e-06504ab3d122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape= (3, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'blessing': 0,\n",
       " 'breather': 1,\n",
       " 'fulfil': 2,\n",
       " 'granted': 3,\n",
       " 'help': 4,\n",
       " 'promise': 5,\n",
       " 'right': 6,\n",
       " 'searching': 7,\n",
       " 'thank': 8,\n",
       " 'times': 9,\n",
       " 've': 10,\n",
       " 'wonderful': 11,\n",
       " 'wont': 12,\n",
       " 'words': 13}"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "print(\"Shape=\", bow.shape)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H77xYv43Lgn5"
   },
   "source": [
    "#### **TF-IDF**\n",
    "\n",
    "[Документация в sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "Мешок слов не учитывает \"веса\" слов, он просто смотрит их вхождение в документ. Вероятно, было бы полезно взвесить каким-то образом каждое слово в документе. Действительно, если слово встречается во всех документах, то, наверное, его вес небольшой. А если редкое слово встречается в некоторых документах, то скорее всего оно какое-то узко тематическое.\n",
    "\n",
    "Один из способов взвесить слова - это использовать меру tf-idf, где:\n",
    "\n",
    "**TF - term frequency** - частота слова для каждой статьи\n",
    "\n",
    "$$\\LARGE \\mathrm{tf}(t,d) = \\frac{n_t}{\\sum_k n_k}$$\n",
    "\n",
    "**IDF - inverse document frequency*** — обратная частота документа - уменьшает вес часто встречаемых слов\n",
    "\n",
    "$$\\LARGE \\mathrm{idf}(t, D) =  \\log \\frac{|D|}{|\\{\\,d_i \\in D \\mid t \\in d_{i}\\, \\}|}$$\n",
    "\n",
    "$|D|$ - число документов в корпусе\n",
    "\n",
    "$|\\{\\,d_i \\in D \\mid t \\in d_{i}\\, \\}|$ - число документов из коллекции ${\\displaystyle D}$ , в которых встречается ${\\displaystyle t}$  (когда ${\\displaystyle n_{t}\\neq 0}$ ).\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "$$\\LARGE \\operatorname{tf-idf}(t,d,D) = \\operatorname{tf}(t,d) \\times \\operatorname{idf}(t, D)$$\n",
    "\n",
    "\n",
    "Синтаксис такой же, как и у мешка слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6uWHcobLgn5",
    "outputId": "0056f278-cfb4-4e98-ccd2-f7b87bbb7f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape= (3, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "texts = [\n",
    "    \"I've been searching for the right words to thank you for this breather.\",\n",
    "    \"You have been wonderful and a blessing at all times\",\n",
    "    \"I promise i wont take your help for granted and will fulfil my promise.\"\n",
    "]\n",
    "bow = tfidf_vectorizer.fit_transform(texts)\n",
    "print(\"Shape=\", bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDwWgta3Lgn6",
    "outputId": "01ce4c11-2912-4587-8e21-a451db93a4bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blessing': 0,\n",
       " 'breather': 1,\n",
       " 'fulfil': 2,\n",
       " 'granted': 3,\n",
       " 'help': 4,\n",
       " 'promise': 5,\n",
       " 'right': 6,\n",
       " 'searching': 7,\n",
       " 'thank': 8,\n",
       " 'times': 9,\n",
       " 've': 10,\n",
       " 'wonderful': 11,\n",
       " 'wont': 12,\n",
       " 'words': 13}"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j23wuTh9Lgn6",
    "outputId": "aa49f5a5-6394-40c8-b415-2f0558c70efc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.40824829, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.40824829, 0.40824829, 0.40824829, 0.        ,\n",
       "        0.40824829, 0.        , 0.        , 0.40824829],\n",
       "       [0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.57735027, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.35355339, 0.35355339, 0.35355339,\n",
       "        0.70710678, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.35355339, 0.        ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khpVDEVgLgn6"
   },
   "source": [
    "#### **Задание**\n",
    "\n",
    "Обратясь к примерам выше и к документации по ссылкам, создайте векторайзеры для подготовленных в предыдущем параграфе данных. Используйте английские стоп-слова. \n",
    "\n",
    "Так как у нас есть две части (тренировочная и тестовая выборки), то векторайзер нужно обучить на словах из обоих выборок (подумайте, почему). Так как у нас питон, а наши выборки это массивы строк, то объеденить их очень просто - просто сложить. После того, как вы обучили векторайзер на всех словах, проведите трансформации отдельно для тестовой, и отдельно для тренировочных частей.\n",
    "\n",
    "Векторизованные части **назовите** `xcv_test/train` для count_vectorizer, и `xTfidf_test/train` для TF-IDF - это нужно для корректной работы тестов и примеров ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "roxobqNsLgn7"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "texts = np.hstack((x_test, x_train))\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "\n",
    "xcv_test = bow[:len(x_test)]\n",
    "xcv_train = bow[len(x_test):]\n",
    "\n",
    "# --------------------------------------\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "bow_2 = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "xTfidf_test = bow_2[:len(x_test)]\n",
    "xTfidf_train = bow_2[len(x_test):]\n",
    "\n",
    "# xTfidf_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9llJqtHLgn7"
   },
   "source": [
    "Небольшой тест на проверку размерностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "phJ24tZqLgn7"
   },
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "assert xcv_train.shape == (1977, 33529)\n",
    "assert xcv_test.shape == (1318, 33529)\n",
    "\n",
    "#tf-idf\n",
    "assert xTfidf_train.shape == (1977, 33529)\n",
    "assert xTfidf_test.shape == (1318, 33529)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JAwiOt_Lgn7"
   },
   "source": [
    "<a name=\"clf-description\"></a>\n",
    "### Реализация классификатора\n",
    "\n",
    "Вспомните статью из блога, которая была в самом начале.\n",
    "\n",
    "Модель классификатора строится на основе обучающей выборки. \n",
    "По ней необходимо найти следующюю статистику:  \n",
    "1. Частоты классов в корпусе объектов (сколько объектов принадлежит каждому из классов)  (`classes_stats`)\n",
    "2. Cуммарное число слов в документах каждого класса (`words_per_class`, далее см. $L_c$)\n",
    "3. Частоты слов в пределах каждого класса (`word_freqs_per_class`, далее используется для расчета $W_{ic}$)\n",
    "4. Размер словаря выборки (число признаков) - кол-во уникальных слов в выборке (`num_features`)\n",
    "\n",
    "По сути, это метод `fit` классификатора.\n",
    "\n",
    "На этапе предсказания необходимо воспользоваться следующей формулой:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "predicted\\ class = \\operatorname*{arg\\max}_{c \\in C} \\left[\\log{{D_c} \\over {D}} + \\sum_{i \\in Q}{\\log{{W_{ic} + 1} \\over {|V| + L_c}}}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Поясним некоторые переменные в этом выражении:  \n",
    "$D_c$ - количество документов в обуч. выборке $\\in$ классу $c$\n",
    "\n",
    "$D$ - сколько всего было документов в обуч. выборке\n",
    "\n",
    "$|V|$ - количество уникальных слов во сех документах обуч. выборки\n",
    "\n",
    "$L_c$ - cуммарное число слов в документах класса $c$ обучающей выборки\n",
    " \n",
    "$W_{ic}$ - сколько раз $i$ слово встретилось в объектах класса $c$ обучающей выборки\n",
    "\n",
    "$Q$ - множество слов _классифицируемого_ документа\n",
    "\n",
    "Сигнатура класса:\n",
    "\n",
    "```python\n",
    "class NaiveBayes:\n",
    "    def fit(self, x, y) -> None\n",
    "    \n",
    "    def predict(self, x) -> List[Int]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOl06mCYLgn8"
   },
   "source": [
    "Для начала отдельно подсчитаем различные статистики, описанные в статье и в материале выше. \n",
    "Так как у нас уже готовы все данные, то считать будем по **count_vectorizer**'у (то есть xcv_ ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xit7_y8jLgn8"
   },
   "source": [
    "Общее число документов в обучающей выборке (`doc_num`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "aHPsWi3eLgn8"
   },
   "outputs": [],
   "source": [
    "doc_num = xcv_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "i6rJBGiKaU-7"
   },
   "outputs": [],
   "source": [
    "# ПРОВЕРКА\n",
    "assert doc_num == 1977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZdjDDTELgn8"
   },
   "source": [
    "Словарь, содержащий число объектов каждого класса (`classes_stats`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "APfAvqcyLgn9"
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "classes_stats = dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "vkyU6KypajSC"
   },
   "outputs": [],
   "source": [
    "# ПРОВЕРКА\n",
    "assert classes_stats['alt.atheism'] == 468\n",
    "assert classes_stats['comp.graphics'] == 571\n",
    "assert classes_stats['sci.space'] == 577\n",
    "assert classes_stats['talk.religion.misc'] == 361"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMPXQjAGLgn9"
   },
   "source": [
    "Число уникальных признаков (слов) в тренировочной выборке (`num_features`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "fbsLa0wiLgn9"
   },
   "outputs": [],
   "source": [
    "num_features = xcv_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "r1PgsyaHanzE"
   },
   "outputs": [],
   "source": [
    "# ПРОВЕРКА\n",
    "assert num_features == 33529"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7kZMoe1Lgn9"
   },
   "source": [
    "Создадим словарь `indexes`, в котором ключом будет являться имя класса, а значением - список строк матрицы X, принадлежащих этому классу. Этот список пригодится нам дальше, так как будет играть роль маски. Для поиска класса каждой из строк используйте целевой вектор.\n",
    "\n",
    "**Hints:**\n",
    "- [np.where](https://numpy.org/doc/stable/reference/generated/numpy.where.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "bbO1neuVLgn-"
   },
   "outputs": [],
   "source": [
    "indexes = {}\n",
    "y_train_np = np.array(y_train)\n",
    "\n",
    "for i in np.unique(y_train_np):\n",
    "    k, =np.where(y_train_np == i)\n",
    "    indexes[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "HmjQZiKLauNb"
   },
   "outputs": [],
   "source": [
    "# ПРОВЕРКА\n",
    "# так как в словаре очень много элементов, то проверим случайные элементы из списка.\n",
    "# если вы все сделали правильно, то эти элементы совпадут.\n",
    "assert indexes['sci.space'][35] == 111\n",
    "assert indexes['comp.graphics'][42] == 159\n",
    "assert indexes['talk.religion.misc'][67] == 312\n",
    "assert indexes['alt.atheism'][89] == 372"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipTcV_I1Lgn-"
   },
   "source": [
    "Используя найденные выше индексы, подсчитаем два важных параметра `words_per_class` и `word_freqs_per_class`.  \n",
    "Обе этих переменных являются словарями, но первая из них отвечает за суммарное число слов, использованных в **каждом классе**, а вторая показывает, сколько раз конкретное слово встретилось в документах определенного класса.  Соответственно, формат переменной `words_per_class` - `{str: int}`, формат `word_freqs_per_class` - `{str: List}`.\n",
    "Мы специально объеденили поиск двух разных статистик в одном блоке, чтобы избежать лишних циклов.\n",
    "\n",
    "Чтобы найти в X строки, относящиеся к тому или иному классу, воспользуйтесь поиском по маске `indexes` для нужного класса.\n",
    "\n",
    "Также помните, что X - это разряженная матрица, но из нее можно получить обычный список через метод `toarray()`\n",
    "\n",
    "**Hints::**\n",
    "- вспомните про маски в numpy-массивах\n",
    "```python\n",
    "mask = [1,0,2] #indexes\n",
    "array = [1,2,4,8,16,32,64,128]\n",
    "array[mask]\n",
    "#result: array([2, 1, 4])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHxuZaP9Lgn-",
    "outputId": "7c03bc42-7e61-4635-e6d6-8fe3cbb7b242"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'alt.atheism': 8737,\n",
       "  'comp.graphics': 10592,\n",
       "  'sci.space': 13273,\n",
       "  'talk.religion.misc': 8860},\n",
       " {'alt.atheism': array([ 0, 17,  0, ...,  0,  0,  0]),\n",
       "  'comp.graphics': array([26, 12,  0, ...,  0,  0,  2]),\n",
       "  'sci.space': array([32, 92,  2, ...,  0,  0,  0]),\n",
       "  'talk.religion.misc': array([1, 9, 0, ..., 0, 0, 0])})"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_arr = xcv_train.toarray()\n",
    "\n",
    "words_per_class = {}\n",
    "word_freqs_per_class = {}\n",
    "\n",
    "for cls in indexes.keys():\n",
    "    class_idxs = indexes[cls] # нашли индексы строк матрицы, относящихся к классу cls\n",
    "\n",
    "    subarray_rows = x_arr[class_idxs] # нашли подмассив, относящийся к  классу cls\n",
    "    subarray_sum = np.sum(subarray_rows, axis = 0) # провели суммирование по столбцам\n",
    "    word_freqs_per_class[cls] = subarray_sum\n",
    "\n",
    "    words_per_class[cls] = len(subarray_sum[subarray_sum != 0]) # узнали,\n",
    "        # сколько слов было использовано в рамках одного класса, \n",
    "        # то есть просто подсчитали число ненулевых элементов\n",
    "\n",
    "words_per_class, word_freqs_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GSxOKIxLgn_"
   },
   "source": [
    "Все вышенаписанные переменные образуют метод `fit` будущего классификатора. Теперь внесите этот код в метод fit, и не забудьте сделать найденные переменные полями экземпляра класса при помощи `self`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "cHVzNkO8Lgn_"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        doc_num = None\n",
    "        classes_stats = None\n",
    "        num_features = None\n",
    "        indexes = None\n",
    "        words_per_class = None\n",
    "        word_freqs_per_class = None\n",
    "        \n",
    "\n",
    "    def fit(self, xcv_train, y_train):\n",
    "        self.doc_num = xcv_train.shape[0] #\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        self.classes_stats = dict(zip(unique, counts)) #\n",
    "        self.num_features = xcv_train.shape[1] # \n",
    "        indexes = {} #\n",
    "        y_train_np = np.array(y_train)\n",
    "\n",
    "        for i in np.unique(y_train_np):\n",
    "            k, =np.where(y_train_np == i)\n",
    "            indexes[i] = k\n",
    "        self.indexes = indexes\n",
    "        \n",
    "        x_arr = xcv_train.toarray()\n",
    "\n",
    "        words_per_class = {}\n",
    "        word_freqs_per_class = {}\n",
    "\n",
    "        for cls in indexes.keys():\n",
    "            class_idxs = indexes[cls] # нашли индексы строк матрицы, относящихся к классу cls\n",
    "\n",
    "            subarray_rows = x_arr[class_idxs] # нашли подмассив, относящийся к  классу cls\n",
    "            subarray_sum = np.sum(subarray_rows, axis = 0) # провели суммирование по столбцам\n",
    "            word_freqs_per_class[cls] = subarray_sum\n",
    "\n",
    "            words_per_class[cls] = len(subarray_sum[subarray_sum != 0]) # узнали,\n",
    "            # сколько слов было использовано в рамках одного класса, \n",
    "            # то есть просто подсчитали число ненулевых элементов\n",
    "\n",
    "        self.words_per_class = words_per_class\n",
    "        self.word_freqs_per_class = word_freqs_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4COT-aNDLgn_"
   },
   "source": [
    "Теперь реализуем метод `predict`. Вспомните еще раз [большую формулу](#clf-description) из начала этого раздела. Если вы внимательно читали статью, то заметили, что в примере мы получили не чистые вероятности классов, а всего лишь числовые оценки. Далее эти оценки можно перевести в вероятности, но мы этого делать не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7dnSPbILgn_"
   },
   "source": [
    "Тогда промежуточный выход классификатора обозначим так:\n",
    "\n",
    "`pred_per_class = {<номер строки в тестовой выборке>: {<класс 1>: <оценка>, ... , <класс n>: <оценка>}}`\n",
    "\n",
    "Таким образом итоговый класс к которому будет отнесена строка - просто класс с наибольшей оценкой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo7EWqLHLgoA"
   },
   "source": [
    "Не забудьте о следующих вещах:\n",
    "- X_test - это такая же разряженная матрица, нужно превратить ее в список\n",
    "- переменная $\\frac{D_c}{D}$ одинакова для одного класса.\n",
    "- Чтобы подсчитать $W_{ic}$ воспользуйтесь `word_freqs_per_class`.\n",
    "- Чтобы понять, какие элементы вам нужно брать в `word_freqs_per_class`, найдите индексы ненулевых элементов в классифицируемой строке - если эти элементы ненулевые, значит, там было какое-то слово\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iws9JGvsdoHl"
   },
   "source": [
    "**Hints:**\n",
    "- [np.nonzero](https://numpy.org/doc/stable/reference/generated/numpy.nonzero.html) (будьте внимательны с типом возвращаемого значения!)\n",
    "- [Math.log](https://docs.python.org/3/library/math.html) для чисел\n",
    "- [np.log](https://numpy.org/doc/stable/reference/generated/numpy.log.html) для массивов\n",
    "- Так как с каждым разом вы добавляете оценку для нового класса, логично использовать [defaultdict(dict)](https://docs.python.org/3/library/collections.html#collections.defaultdict): так создается словарь, состоящий из пустых словарей, и при обращении к элементу словаря, например `pred_per_class['SomeID']` мы получаем словарь, для которого доступны все стандартные методы (например, update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "_o7Oz-WfLgoA"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def predict(x):\n",
    "    x = x.toarray()\n",
    "    # pred_per_class = {<номер строки в тестовой выборке>: {<класс 1>: <оценка>, ... , <класс n>: <оценка>}}\n",
    "    pred_per_class = defaultdict(dict)\n",
    "    D = doc_num\n",
    "    mV = num_features # ?\n",
    "    pred = []\n",
    "    for i in x:\n",
    "        min_ = float('-inf')\n",
    "        ind_nonzero_items = np.nonzero(i)\n",
    "        # print(ind_nonzero_items)\n",
    "        for clas in classes_stats:\n",
    "            # print(clas) # clas\n",
    "            Dc = classes_stats[clas]\n",
    "            Lc = words_per_class[clas] # c = class      \n",
    "                \n",
    "            slag = 0\n",
    "            for e in ind_nonzero_items[0]:\n",
    "                slag += math.log((word_freqs_per_class[clas][e] + 1)/(mV + Lc))\n",
    "            # Wic = word_freqs_per_class[clas][np.nonzero(word_freqs_per_class[clas])] # ?\n",
    "            #print(self.word_freqs_per_class[clas])\n",
    "                \n",
    "            r = math.log(Dc/D) + slag # формула\n",
    "            if (r > min_):\n",
    "                min_ = r\n",
    "                app = clas\n",
    "            pred_per_class[i[0]] = {clas:r}\n",
    "                \n",
    "        pred.append(app)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "vvuxVe5TLgoA"
   },
   "outputs": [],
   "source": [
    "# проверим размерность предсказаний.\n",
    "assert len(predict(xcv_test)) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NnmiBrGLgoA"
   },
   "source": [
    "Теперь соберем весь классфикатор вместе, внеся функцию `predict` внутрь ранее написанного класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "oPdwpkuKLgoB"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        doc_num = None\n",
    "        classes_stats = None\n",
    "        num_features = None\n",
    "        indexes = None\n",
    "        words_per_class = None\n",
    "        word_freqs_per_class = None\n",
    "        \n",
    "\n",
    "    def fit(self, xcv_train, y_train):\n",
    "        self.doc_num = xcv_train.shape[0] #\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        self.classes_stats = dict(zip(unique, counts)) #\n",
    "        self.num_features = xcv_train.shape[1] # \n",
    "        indexes = {} #\n",
    "        y_train_np = np.array(y_train)\n",
    "\n",
    "        for i in np.unique(y_train_np):\n",
    "            k, =np.where(y_train_np == i)\n",
    "            indexes[i] = k\n",
    "        self.indexes = indexes\n",
    "        \n",
    "        x_arr = xcv_train.toarray()\n",
    "\n",
    "        words_per_class = {}\n",
    "        word_freqs_per_class = {}\n",
    "\n",
    "        for cls in indexes.keys():\n",
    "            class_idxs = indexes[cls] # нашли индексы строк матрицы, относящихся к классу cls\n",
    "\n",
    "            subarray_rows = x_arr[class_idxs] # нашли подмассив, относящийся к  классу cls\n",
    "            subarray_sum = np.sum(subarray_rows, axis = 0) # провели суммирование по столбцам\n",
    "            word_freqs_per_class[cls] = subarray_sum\n",
    "\n",
    "            words_per_class[cls] = len(subarray_sum[subarray_sum != 0]) # узнали,\n",
    "            # сколько слов было использовано в рамках одного класса, \n",
    "            # то есть просто подсчитали число ненулевых элементов\n",
    "\n",
    "        self.words_per_class = words_per_class\n",
    "        self.word_freqs_per_class = word_freqs_per_class\n",
    "        \n",
    "    def predict(self,x):\n",
    "        x = x.toarray()\n",
    "        # pred_per_class = {<номер строки в тестовой выборке>: {<класс 1>: <оценка>, ... , <класс n>: <оценка>}}\n",
    "        pred_per_class = defaultdict(dict)\n",
    "        D = self.doc_num\n",
    "        mV = self.num_features # ?\n",
    "        pred = []\n",
    "        for i in x:\n",
    "            min_ = float('-inf')\n",
    "            ind_nonzero_items = np.nonzero(i)\n",
    "            # print(ind_nonzero_items)\n",
    "            for clas in self.classes_stats:\n",
    "                # print(clas) # clas\n",
    "                Dc = self.classes_stats[clas]\n",
    "                Lc = self.words_per_class[clas] # c = class      \n",
    "                \n",
    "                slag = 0\n",
    "                for e in ind_nonzero_items[0]:\n",
    "                    slag += math.log((word_freqs_per_class[clas][e] + 1)/(mV + Lc))\n",
    "                # Wic = word_freqs_per_class[clas][np.nonzero(word_freqs_per_class[clas])] # ?\n",
    "                #print(self.word_freqs_per_class[clas])\n",
    "                \n",
    "                r = math.log(Dc/D) + slag # формула\n",
    "                if (r > min_):\n",
    "                    min_ = r\n",
    "                    app = clas\n",
    "                pred_per_class[i[0]] = {clas:r}\n",
    "                \n",
    "            pred.append(app)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJyBqIc7fo10"
   },
   "source": [
    "### Проверка классификатора\n",
    "\n",
    "Воспользуемся матрицей ошибок и отчетом классификации из `sklearn`, проверим классификатор и на count_vectorizer-векторах, и на tf-idf-векторах. Для начала поговорим о способах оценки качества классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPpD5E193W0B"
   },
   "source": [
    "#### Матрица ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4EgZ5ow3b3Q"
   },
   "source": [
    "Основной материал можете прочесть в [статье](https://habr.com/ru/company/ods/blog/328372/), тут же напишем кратко."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osbSv3zY3xDU"
   },
   "source": [
    "Рассмотрим матрицу: \n",
    " \n",
    " \n",
    " <br></br> | $y=1$ | $y=0$\n",
    "--- | --- | ---\n",
    "$\\overline{y} = 1$ | **True Positive (TP)** | **False Positive (FP)**\n",
    "$\\overline{y} = 0$ | **False Negative (FN)** | **True Negative (TN)**\n",
    "\n",
    "$y$ - истинная метка класса, $\\overline{y}$ - предсказание классификатора.\n",
    "\n",
    "По главной диагонали матрицы - число правильно классифицированных объектов (TP и FP).  \n",
    "\n",
    "Понятно, что число классов может быть больше двух, но для простоты будем рассматривать только случай с двумя классами, назовем их условно Positive и Negative.\n",
    "\n",
    "Ошибки бывают двух типов: **ошибки первого рода** (FP), или ложноположительное срабатывание, когда, например, анализ показывает заболевание, хотя на самом деле человек здоров, и **ошибки второго рода** (FN), или пропуск события, когда больного человека принимают за здорового. \n",
    "\n",
    "В некотрых случаях по оси X может отображаться предсказание модели, а по оси Y - истинные метки. Важно не запоминать положение элементов в матрице, а в зависимости от заданных осей понимать, где ошибки, а где правильные значения.\n",
    "\n",
    "**Метрики**\n",
    "\n",
    "_Accuracy_, или точность.  \n",
    "Обозначает в целом долю правильных ответов:\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\n",
    "$$\n",
    "\n",
    "Недостаток метрики в том, что она не работает на несбалансированных выборках.\n",
    "\n",
    "_Precision_ и _Recall_, или точность и полнота.  \n",
    "\n",
    "---\n",
    "\n",
    "**Важное замечание**  \n",
    "_Оба термина переводятся на русский язык как точность, хотя при этом отражают разные понятия. Чтобы не допустить недопонимая, можно использовать английские слова напрямую, либо accuracy называть аккуратностью, а precision - точностью. Полнота - она и есть полнота._\n",
    "\n",
    "---\n",
    "\n",
    "Эти две метрики рассчитываются отдельного для каждого из классов. Для примера рассмотрим класс Positive:\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Precision отражает долю объектов класса Positive, которые классификатор классифицировал верно.\n",
    "\n",
    "Recall показывает долю объектов класса Positive среди всех объектов класса Positive, которые вообще нашел алгоритм.\n",
    "\n",
    "F-мера - один из способов объеденить Precision и Recall:\n",
    "\n",
    "$$\n",
    "F_\\beta = (1 + \\beta^2) \\cdot \\frac{Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall}\n",
    "$$\n",
    "\n",
    "$\\beta$ - вес Precision в метрике.\n",
    "\n",
    "В `sklearn` за матрицу ошибок и метрики отвечают [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) и [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BKyh41ZLgoB",
    "outputId": "20fbcd62-5ff6-4643-bbfb-a4ec75b0ad7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.62      0.74      0.67       311\n",
      "     comp.graphics       0.91      0.90      0.90       384\n",
      "         sci.space       0.77      0.90      0.83       378\n",
      "talk.religion.misc       0.72      0.37      0.49       245\n",
      "\n",
      "          accuracy                           0.76      1318\n",
      "         macro avg       0.76      0.73      0.72      1318\n",
      "      weighted avg       0.77      0.76      0.75      1318\n",
      "\n",
      "[[229   9  41  32]\n",
      " [  9 344  31   0]\n",
      " [ 16  17 342   3]\n",
      " [117   8  29  91]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "nb = NaiveBayes()\n",
    "\n",
    "nb.fit(xcv_train, y_train)\n",
    "pred = nb.predict(xcv_test)\n",
    "        \n",
    "print(classification_report(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZr0BWzTFm71"
   },
   "source": [
    "Проверим классификатор на полученных ранее tf-idf векторах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYA5YltjLgoB",
    "outputId": "796e0979-b4b1-41b8-c882-d34cbfe4a463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.62      0.74      0.67       311\n",
      "     comp.graphics       0.91      0.90      0.90       384\n",
      "         sci.space       0.77      0.90      0.83       378\n",
      "talk.religion.misc       0.72      0.37      0.49       245\n",
      "\n",
      "          accuracy                           0.76      1318\n",
      "         macro avg       0.76      0.73      0.72      1318\n",
      "      weighted avg       0.77      0.76      0.75      1318\n",
      "\n",
      "[[229   9  41  32]\n",
      " [  9 344  31   0]\n",
      " [ 16  17 342   3]\n",
      " [117   8  29  91]]\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "\n",
    "nb.fit(xTfidf_train, y_train)\n",
    "pred = nb.predict(xTfidf_test)\n",
    "        \n",
    "print(classification_report(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV-8MaAjNEGn"
   },
   "source": [
    "### Сравним с версией из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcEkWenYLgoC",
    "outputId": "02f50efa-fed5-4c2e-c8af-b84fb8cb5d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.62      0.75      0.68       311\n",
      "     comp.graphics       0.91      0.91      0.91       384\n",
      "         sci.space       0.83      0.89      0.86       378\n",
      "talk.religion.misc       0.68      0.42      0.52       245\n",
      "\n",
      "          accuracy                           0.78      1318\n",
      "         macro avg       0.76      0.74      0.74      1318\n",
      "      weighted avg       0.78      0.78      0.77      1318\n",
      "\n",
      "[[232   9  28  42]\n",
      " [ 12 351  20   1]\n",
      " [ 20  18 335   5]\n",
      " [112   9  20 104]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=4)\n",
    "clf.fit(xcv_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(xcv_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCo-Zg6aNBFY",
    "outputId": "6a4f1e73-4a32-4880-cda4-a4420690414e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.60      0.64      0.62       311\n",
      "     comp.graphics       0.84      0.93      0.88       384\n",
      "         sci.space       0.63      0.92      0.75       378\n",
      "talk.religion.misc       0.85      0.04      0.09       245\n",
      "\n",
      "          accuracy                           0.69      1318\n",
      "         macro avg       0.73      0.63      0.58      1318\n",
      "      weighted avg       0.73      0.69      0.63      1318\n",
      "\n",
      "[[199  22  88   2]\n",
      " [  1 356  27   0]\n",
      " [  5  24 349   0]\n",
      " [126  20  88  11]]\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=4)\n",
    "clf.fit(xTfidf_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(xTfidf_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPQbSuomieol"
   },
   "source": [
    "### Порассуждаем (дополнительно)\n",
    "\n",
    "Как вы заметили, качество предсказаний неоднородно (какие-то классы определяются хорошо, какие-то не очень хорошо). В чем может быть причина этого? \n",
    "\n",
    "Самостоятельно попробуйте найти топ-10 наиболее часто встречающихся слов в каждой категории, посмотрите на эти слова и напишите свои идеи, почему же при классификации точность падает на той или иной категории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_2z_s2-jq_f"
   },
   "source": [
    "**_Поле для ответа_**\n",
    "\n",
    " . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1NzFNBMjpp3",
    "outputId": "53fd9043-7726-4187-b973-e1b12a4b3a73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism : like; don; believe; just; atheism; does; think; people; say; god; \n",
      "\n",
      "comp.graphics : graphics; software; jpeg; use; image; images; file; edu; data; files; \n",
      "\n",
      "sci.space : like; space; just; time; nasa; data; shuttle; earth; orbit; launch; \n",
      "\n",
      "talk.religion.misc : don; know; just; think; people; jesus; christian; say; god; bible; \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = xcv_train.toarray()\n",
    "d = []\n",
    "\n",
    "for i in indexes:\n",
    "    data = []\n",
    "    for z in indexes[i]:\n",
    "        data.append(temp[z])\n",
    "\n",
    "    ex = np.sum(data,axis=0)\n",
    "    d.append(np.argsort(ex)[-10:].tolist())\n",
    "    \n",
    "k = 0\n",
    "for i in indexes:\n",
    "    print(i,':', end=\" \")\n",
    "    z = 0\n",
    "    for j in count_vectorizer.vocabulary_:\n",
    "        for h in d[k]:\n",
    "            if (h == count_vectorizer.vocabulary_[j]):\n",
    "                print(j, end=\"; \")\n",
    "        z += 1\n",
    "    k += 1\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "zoHB7oYlIALZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NAIVE_BAYES (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
